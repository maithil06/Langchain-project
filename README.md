# ğŸ“š Personal RAG System for Technical Books

A Retrieval-Augmented Generation (RAG) system built to query and refresh technical concepts directly from a personal collection of Oâ€™Reilly and other ML/AI books.  
Instead of manually searching PDFs, this system allows you to ask natural language questions and receive **contextual, referenced answers** from the original source material.

---

## ğŸš€ Why This Project?

While working on multiple Machine Learning and GenAI projects, I often remembered studying a concept earlier in one of my books but had to waste time manually searching chapters to find it again.

This project solves that problem by turning your technical library into a **queryable knowledge base**.

---

## ğŸ§  What It Does

- Ingests technical books and documents (currently supports PDF)
- Chunks content for efficient retrieval
- Stores and uses **book-level metadata** for precise referencing
- Retrieves relevant context and answers questions with citations
- Designed to scale across multiple data sources and formats

---

## ğŸ› ï¸ Tech Stack

- **LangChain** â€“ Pipeline orchestration and RAG framework  
- **Sentence Transformers** â€“ Semantic embeddings  
- **ChromaDB** â€“ Vector database for fast retrieval  
- **Git** â€“ Version control

---

## ğŸ“‚ Project Structure

Langchain-project/
â”œâ”€â”€ data/                  # Input documents (PDFs)
â”œâ”€â”€ chunks/                # Preprocessed text chunks
â”œâ”€â”€ metadata/              # Book-level metadata
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ ingestion.py       # Document loading and preprocessing
â”‚   â”œâ”€â”€ chunking.py        # Text chunking pipeline
â”‚   â”œâ”€â”€ retrieval.py      # RAG retrieval logic
â”‚   â””â”€â”€ app.py             # Main application entry point
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md


